{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXbCtaitQ_Ss"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from math import exp\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qHbNio2jHnl",
        "outputId": "a6127966-a8c9-4424-fc06-5bd9750e1606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "GicSI59GgiNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUaejoURd1Fm"
      },
      "outputs": [],
      "source": [
        "# State #\n",
        "BOARD_SIZE = (3,3)\n",
        "\n",
        "# Agent #\n",
        "C_PUCT_INIT = 2.5\n",
        "C_PUCT_DECAY = 0.05\n",
        "C_PUCT_MIN = 1.0\n",
        "\n",
        "TAU_INIT = 1.0\n",
        "TAU_DECAY = 0.05\n",
        "TAU_MIN = 1e-5\n",
        "\n",
        "NUM_OF_SIMULATION = 200\n",
        "\n",
        "# Main #\n",
        "NUM_EPOCHS = 2 # (self play 데이터 이용한) 학습 횟수\n",
        "NUM_GAMES = 10 # self play 횟수\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "LEARNING_RATE = 0.0005\n",
        "WEIGHT_DECAY = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjF-DPiDo7yP"
      },
      "source": [
        "## State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_OHKemkBy7N"
      },
      "outputs": [],
      "source": [
        "class State:\n",
        "    def __init__(self, board_size=BOARD_SIZE, my_actions=None, enemy_actions=None):\n",
        "        self.board_size = board_size # (3,3)\n",
        "        self.num_actions = self.board_size[0] * self.board_size[1] # 3 * 3 = 9\n",
        "        self.action_space = range(self.num_actions)\n",
        "\n",
        "        self.my_actions = [] if my_actions is None else my_actions\n",
        "        self.enemy_actions = [] if enemy_actions is None else enemy_actions\n",
        "\n",
        "        self.board = self._create_board(self.my_actions, self.enemy_actions)\n",
        "\n",
        "        self.available_actions = self._get_available_actions()\n",
        "\n",
        "    def next(self, action):\n",
        "        '''\n",
        "        내 행동 이후 상대방 턴으로 변경\n",
        "        '''\n",
        "        my_actions = self.my_actions.copy()\n",
        "        my_actions.append(action)\n",
        "\n",
        "        return State(self.board_size, self.enemy_actions, my_actions)\n",
        "\n",
        "    def _create_board(self, my_actions, enemy_actions):\n",
        "        total_board = np.zeros((2, self.board_size[0], self.board_size[1]))\n",
        "\n",
        "        my_board = np.zeros(self.board_size).flatten()\n",
        "        enemy_board = np.zeros(self.board_size).flatten()\n",
        "\n",
        "        my_board[my_actions] = 1\n",
        "        enemy_board[enemy_actions] = 1\n",
        "\n",
        "        total_board[0] = my_board.reshape(self.board_size)\n",
        "        total_board[1] = enemy_board.reshape(self.board_size)\n",
        "\n",
        "        return total_board\n",
        "\n",
        "    def _get_available_actions(self):\n",
        "        my_actions_set = set(self.my_actions)\n",
        "        enemy_actions_set = set(self.enemy_actions)\n",
        "\n",
        "        available_actions_set = set(self.action_space) - my_actions_set - enemy_actions_set\n",
        "\n",
        "        return list(available_actions_set)\n",
        "\n",
        "    def is_win(self):\n",
        "        my_state = self.board[0]\n",
        "\n",
        "        row_win = np.sum(my_state, axis=0).max() == self.board_size[0]\n",
        "        col_win = np.sum(my_state, axis=1).max() == self.board_size[1]\n",
        "        diag_win = np.trace(my_state) == self.board_size[0]\n",
        "        anti_diag_win = np.trace(np.fliplr(my_state)) == self.board_size[0]\n",
        "\n",
        "        return row_win or col_win or diag_win or anti_diag_win\n",
        "\n",
        "    def is_draw(self):\n",
        "        return (np.sum(self.board[0]) + np.sum(self.board[1])) >= self.num_actions\n",
        "\n",
        "    def is_lose(self):\n",
        "        enemy_state = self.board[1]\n",
        "\n",
        "        row_lose = np.sum(enemy_state, axis=0).max() == self.board_size[0]\n",
        "        col_lose = np.sum(enemy_state, axis=1).max() == self.board_size[1]\n",
        "        diag_lose = np.trace(enemy_state) == self.board_size[0]\n",
        "        anti_diag_lose = np.trace(np.fliplr(enemy_state)) == self.board_size[0]\n",
        "\n",
        "        return row_lose or col_lose or diag_lose or anti_diag_lose\n",
        "\n",
        "    def is_done(self):\n",
        "        return self.is_win() or self.is_draw() or self.is_lose()\n",
        "\n",
        "    def is_going_first(self):\n",
        "        return len(self.my_actions) == len(self.enemy_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvECaRBo9s1"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkZ47mFvAAeX"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEnv:\n",
        "    def __init__(self):\n",
        "        self.state = State()\n",
        "        self.reward = {'win': 10, 'lose': -10, 'draw': 0, 'continue': 0}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = State()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        my_actions = self.state.my_actions.copy()\n",
        "        enemy_actions = self.state.enemy_actions.copy()\n",
        "\n",
        "        my_actions.append(action)\n",
        "\n",
        "        next_state = State(BOARD_SIZE, my_actions, enemy_actions) # 현재 턴 끝난 상태 반영\n",
        "        self.state = State(BOARD_SIZE, self.state.enemy_actions, my_actions) # 현재 내 턴 끝나고 상대 턴으로 변경\n",
        "\n",
        "        if next_state.is_win():\n",
        "            reward, done = self.reward['win'], True\n",
        "\n",
        "        elif next_state.is_draw():\n",
        "            reward, done = self.reward['draw'], True\n",
        "\n",
        "        elif next_state.is_lose():\n",
        "            reward, done = self.reward['lose'], True\n",
        "\n",
        "        else:\n",
        "            reward, done = self.reward['continue'], False\n",
        "\n",
        "        return self.state, next_state, reward, done\n",
        "\n",
        "    def render(self, state):\n",
        "        board = state.board[0] - state.board[1] if state.is_going_first() else state.board[1] - state.board[0]\n",
        "        int_to_symbol = np.vectorize({1: 'O', -1: 'X', 0: '-'}.__getitem__)\n",
        "        rendering_board = int_to_symbol(board)\n",
        "        print(\"\\n\" + \"\\n\".join(\" \".join(row) for row in rendering_board) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WFnHOTiejov"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4uvoFC472mo"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj9ckVV477yg"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, zero_init_residual=False):\n",
        "        super().__init__()\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "\n",
        "        # Initial Conv Layer\n",
        "        self.conv1 = nn.Conv2d(2, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Residual Layers\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1])\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        for i in range(n_blocks):\n",
        "            if i == 0:\n",
        "                layers.append(block(self.in_channels, channels, downsample=(self.in_channels != channels)))\n",
        "            else:\n",
        "                layers.append(block(channels, channels))\n",
        "            self.in_channels = channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avgpool(x)  # Output shape: (batch_size, channels, 1, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw5vkm3D8CiK"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\n",
        "\n",
        "config = ResNetConfig(block=BasicBlock, n_blocks=[2, 2], channels=[16, 16])\n",
        "\n",
        "class AlphaZeroNet(nn.Module):\n",
        "    def __init__(self, board_size=(3, 3), config=config):\n",
        "        super(AlphaZeroNet, self).__init__()\n",
        "        _, _, channels = config\n",
        "        self.board_size = board_size\n",
        "\n",
        "        self.resnet = ResNet(config=config)\n",
        "\n",
        "        # Policy Head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Conv2d(channels[-1], 2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2, board_size[0] * board_size[1]),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Value Head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Conv2d(channels[-1], 1, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)  # Shape: (batch_size, channels, 1, 1)\n",
        "\n",
        "        # Policy Head\n",
        "        policy = self.policy_head(x)\n",
        "\n",
        "        # Value Head\n",
        "        value = self.value_head(x)\n",
        "\n",
        "        return policy, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QoTSWv0QGel"
      },
      "source": [
        "## MCTS Node"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, state, agent, parent=None, action=None, p=None):\n",
        "        self.agent = agent\n",
        "        self.parent_node = parent\n",
        "        self.child_nodes = []\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.n = 0 # 시행 횟수\n",
        "        self.w = 0 # 보상 누계 (누적된 가치)\n",
        "        self.p = p # 해당 액션을 선택할 확률 from policy net\n",
        "\n",
        "    def expand(self):\n",
        "        if not self.is_fully_expanded():\n",
        "            self.child_nodes = [\n",
        "                Node(\n",
        "                    state=self.state.next(action),\n",
        "                    agent=self.agent,\n",
        "                    parent=self,\n",
        "                    action=action,\n",
        "                    p=self.agent.get_prior_prob(self.state, action)\n",
        "                )\n",
        "                for action in self.state.available_actions\n",
        "            ]\n",
        "\n",
        "    def select(self):\n",
        "        if not self.is_fully_expanded():\n",
        "            return self\n",
        "\n",
        "        if not self.child_nodes:\n",
        "            return None\n",
        "\n",
        "        child_visits = np.array([child.n for child in self.child_nodes])\n",
        "        child_values = np.array([-child.w / max(child.n, 1) for child in self.child_nodes])\n",
        "        child_priors = np.array([child.p for child in self.child_nodes])\n",
        "\n",
        "        q_values = child_values\n",
        "        u_values = (\n",
        "            self.agent.c_puct\n",
        "            * child_priors\n",
        "            * np.sqrt(self.n)\n",
        "            / (1 + child_visits)\n",
        "        )\n",
        "\n",
        "        ucb_scores = q_values + u_values\n",
        "        best_child_idx = np.argmax(ucb_scores)\n",
        "\n",
        "        return self.child_nodes[best_child_idx]\n",
        "\n",
        "    def backup(self, value):\n",
        "        node = self\n",
        "        while node is not None:\n",
        "            node.n += 1\n",
        "            node.w += value\n",
        "            node = node.parent_node\n",
        "            value = -value\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.child_nodes) == len(self.state.available_actions)\n",
        "\n",
        "    def get_terminal_value(self):\n",
        "        if self.state.is_lose():\n",
        "            return -1  # 패배\n",
        "        if self.state.is_draw():\n",
        "            return 0  # 무승부\n",
        "        return 1  # 승리"
      ],
      "metadata": {
        "id": "VsueNsnAJ1IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhBf-A_pAQs"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9d54yFpe8NX"
      },
      "outputs": [],
      "source": [
        "class AlphaZeroAgent:\n",
        "    def __init__(self, network, env):\n",
        "        self.network = network\n",
        "        self.env = env\n",
        "\n",
        "        self.num_simulations = NUM_OF_SIMULATION\n",
        "        self.tau = TAU_INIT # temperature\n",
        "        self.c_puct = C_PUCT_INIT\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        root = Node(state=state, agent=self)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "\n",
        "            # Selection\n",
        "            while node.is_fully_expanded() and not node.state.is_done():\n",
        "                node = node.select()\n",
        "\n",
        "            value = 0\n",
        "\n",
        "            # Expansion and Evaluation\n",
        "            if not node.state.is_done():\n",
        "                node.expand()\n",
        "                value = self._predict_value(node.state)\n",
        "            else:\n",
        "                value = node.get_terminal_value()\n",
        "\n",
        "            # Backpropagation\n",
        "            node.backup(value)\n",
        "\n",
        "        action_probs = np.zeros(self.env.state.num_actions)\n",
        "        for child in root.child_nodes:\n",
        "            action_probs[child.action] = child.n\n",
        "\n",
        "        if self.tau == 0:\n",
        "            best_action = np.argmax(action_probs)\n",
        "            action_probs = np.zeros_like(action_probs)\n",
        "            action_probs[best_action] = 1.0\n",
        "        else:\n",
        "            action_probs = action_probs ** (1.0 / self.tau)\n",
        "            action_probs = action_probs / np.sum(action_probs)\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "    def _compute_policy(self, root_node):\n",
        "        policy = np.zeros(self.env.state.num_actions, dtype=np.float32)\n",
        "        for child in root_node.child_nodes:\n",
        "            policy[child.action] = child.n\n",
        "        print()\n",
        "\n",
        "        policy = np.clip(policy, 1e-8, None)\n",
        "\n",
        "        if self.tau == 0:\n",
        "            # tau가 0인 경우 argmax로 결정 (test)\n",
        "            best_action = np.argmax(policy)\n",
        "            final_policy = np.zeros_like(policy)\n",
        "            final_policy[best_action] = 1.0\n",
        "            return final_policy\n",
        "\n",
        "        # tau > 0인 경우 정규화\n",
        "        tau = max(self.tau, 1e-8)\n",
        "        policy = policy ** (1 / tau)\n",
        "        return policy / np.sum(policy)\n",
        "\n",
        "    def _simulate(self, node):\n",
        "        if node.state.is_done():\n",
        "            return node, node.get_terminal_value()\n",
        "\n",
        "        if not node.is_fully_expanded():\n",
        "            node.expand()\n",
        "            value = self._predict_value(node.state)\n",
        "            return node, value\n",
        "\n",
        "        best_child = node.select()\n",
        "        # 재귀적으로 시뮬레이션 수행하되, 자식 노드의 관점에서는 value가 반전됨\n",
        "        leaf_node, value = self._simulate(best_child)\n",
        "        return leaf_node, -value  # 부모 노드 관점에서는 value를 반전\n",
        "\n",
        "    def get_prior_prob(self, state, action):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state.board, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            policy, _ = self.network(state_tensor)\n",
        "            policy = policy.squeeze(0).cpu()\n",
        "        return policy[action]\n",
        "\n",
        "    def _predict_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state.board, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            _, value = self.network(state_tensor)\n",
        "        return value.item()\n",
        "\n",
        "    def random_action(self, state):\n",
        "        return np.random.choice(state.available_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Nyf43dgpHv"
      },
      "source": [
        "## Self-play"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(알파제로에서) 셀프플레이 25,000 게임, MCTS 시뮬레이션 1,600번\n",
        "\n",
        "각 게임마다 처음 30번은 temp = 1, 나머지 move는 temp → 0"
      ],
      "metadata": {
        "id": "TCz9U3gPEKic"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARAI_eckZj2O"
      },
      "outputs": [],
      "source": [
        "def self_play(env, agent):\n",
        "    states, mcts_policies, values = [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    while not state.is_done():\n",
        "        policy = agent.get_policy(state)\n",
        "        action = np.random.choice(len(policy), p=policy)\n",
        "        states.append(state.board)\n",
        "        mcts_policies.append(policy)\n",
        "        state, _, _, _ = env.step(action)\n",
        "\n",
        "    if state.is_win():\n",
        "        outcome = 1\n",
        "    elif state.is_draw():\n",
        "        outcome = 0\n",
        "    else:\n",
        "        outcome = -1\n",
        "\n",
        "    for _ in range(len(states)):\n",
        "        values.append(outcome)\n",
        "        outcome = -outcome  # 상대방의 결과는 반대가 됨\n",
        "\n",
        "    return states, mcts_policies, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_89tSXZgvbm"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xat5X0jhgwoZ"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer, data, batch_size=20):\n",
        "    network.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    states = torch.tensor(np.concatenate([d[0] for d in data]), dtype=torch.float32).to(device)\n",
        "    policies = torch.tensor(np.concatenate([d[1] for d in data]), dtype=torch.float32).to(device)\n",
        "    values = torch.tensor(np.concatenate([d[2] for d in data]), dtype=torch.float32).to(device)\n",
        "\n",
        "    # 데이터셋과 DataLoader 생성\n",
        "    dataset = TensorDataset(states, policies, values)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for batch_states, batch_policies, batch_values in dataloader:\n",
        "        batch_states = batch_states.to(device)\n",
        "        batch_policies = batch_policies.to(device)\n",
        "        batch_values = batch_values.to(device)\n",
        "\n",
        "        pred_policies, pred_values = network(batch_states)\n",
        "\n",
        "        pred_policies = torch.softmax(pred_policies, dim=-1)\n",
        "        policy_loss = -torch.sum(batch_policies * torch.log(pred_policies + 1e-6), dim=1).mean()\n",
        "        value_loss = ((batch_values - pred_values.view(-1)) ** 2).mean()\n",
        "\n",
        "        loss = policy_loss + value_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9H8AjyTZVv1"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "→ 메서드 이름 변경 (기존의 best net이랑 현재 net 대결해서 현재 net 승률 (55%) 넘으면 best net 업데이트)\n",
        "\n",
        "+ temp = 0 으로 설정해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKCuyncvwAZM"
      },
      "outputs": [],
      "source": [
        "def evaluate_network(best_net, current_net, env, num_games=50):\n",
        "    current_agent = AlphaZeroAgent(current_net, env)\n",
        "    best_agent = AlphaZeroAgent(best_net, env)\n",
        "\n",
        "    current_net_wins, best_net_wins, draws = 0, 0, 0\n",
        "\n",
        "    for i in range(num_games):\n",
        "        state = env.reset()\n",
        "\n",
        "        while not state.is_done():\n",
        "            if state.is_going_first():\n",
        "                # 현재 네트워크의 행동\n",
        "                policy = current_agent.get_policy(state)\n",
        "            else:\n",
        "                # 이전 베스트 네트워크의 행동\n",
        "                policy = best_agent.get_policy(state)\n",
        "\n",
        "            action = np.random.choice(len(policy), p=policy)\n",
        "            state, _, _, _ = env.step(action)\n",
        "\n",
        "        # 게임 결과 확인\n",
        "        if state.is_win():\n",
        "            if state.is_going_first():\n",
        "                current_net_wins += 1\n",
        "            else:\n",
        "                best_net_wins += 1\n",
        "        elif state.is_draw():\n",
        "            draws += 1\n",
        "        else:\n",
        "            if state.is_going_first():\n",
        "                best_net_wins += 1\n",
        "            else:\n",
        "                current_net_wins += 1\n",
        "\n",
        "    print(f\"Evaluation: Current Net Wins: {current_net_wins}, Best Net Wins: {best_net_wins}, Draws: {draws}\")\n",
        "    return current_net_wins, best_net_wins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeu86H1ypHlx"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N84y_FfRNUg",
        "outputId": "f4a4ebbd-5493-44a4-acf1-b06533ff8542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, file_path):\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }, file_path)\n",
        "    print(f\"Checkpoint saved\\n\")"
      ],
      "metadata": {
        "id": "_mkVCeSZOHjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        checkpoint = torch.load(file_path)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # 다음 에포크부터 시작\n",
        "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        print(\"No checkpoint found. Starting from scratch.\")\n",
        "    return start_epoch"
      ],
      "metadata": {
        "id": "dia344AVOJxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try_num = input()\n",
        "\n",
        "env = TicTacToeEnv()\n",
        "net = AlphaZeroNet().to(device)\n",
        "agent = AlphaZeroAgent(net, env)\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "best_net = AlphaZeroNet().to(device)\n",
        "best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/kanghwa'\n",
        "file_path = os.path.join(folder_path, f'checkpoint{try_num}.pth')\n",
        "start_epoch = load_checkpoint(net, optimizer, file_path)\n",
        "\n",
        "losses = []\n",
        "win_rates = []\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    data = []\n",
        "    for i in range(NUM_GAMES):\n",
        "        if i <= (NUM_GAMES * 0.1):\n",
        "            agent.c_puct = C_PUCT_INIT\n",
        "            agent.tau = TAU_INIT\n",
        "        else:\n",
        "            agent.c_puct = max(C_PUCT_MIN, C_PUCT_INIT*exp(-C_PUCT_DECAY*i))\n",
        "            agent.tau = max(TAU_MIN, TAU_INIT*exp(-TAU_DECAY*i))\n",
        "\n",
        "        ###\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"C_puct : {agent.c_puct}\")\n",
        "            print(f\"Tau : {agent.tau}\")\n",
        "            print(f\"{i + 1}/{NUM_GAMES}\\n\")\n",
        "        ###\n",
        "\n",
        "        game_data = self_play(env, agent)\n",
        "        data.append(game_data)\n",
        "\n",
        "    loss = train(net, optimizer, data, batch_size=BATCH_SIZE)\n",
        "    losses.append(loss)\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {loss}\")\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        current_net_wins, best_net_wins = evaluate_network(best_net, net, env, NUM_GAMES)\n",
        "        win_rate = current_net_wins / NUM_GAMES\n",
        "        win_rates.append(win_rate)\n",
        "\n",
        "        if win_rate >= 0.55:\n",
        "            print(\"Update best network.\")\n",
        "            best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    save_checkpoint(best_net, optimizer, epoch, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e068Ji8bOKcn",
        "outputId": "3d293400-8ce4-4741-e8e4-1862e7d48f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "잘됨\n",
            "Checkpoint loaded. Resuming from epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b83b391fbe96>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(losses) + 1), losses, label='Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(2, len(win_rates) * 2 + 1, 2), win_rates, label='Win Rate')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9STbzi-IIpWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "y7iLsFmIOizJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcHfCQm_Cgch"
      },
      "outputs": [],
      "source": [
        "def test(agent, env, num_games=10, human_play=False, random_first=True):\n",
        "    player = ''\n",
        "\n",
        "    agent_wins = 0\n",
        "    draws = 0\n",
        "    losses = 0\n",
        "\n",
        "    agent.tau = 0\n",
        "\n",
        "    for game in range(num_games):\n",
        "        print(f\"GAME {game + 1}/{num_games}\")\n",
        "        state = env.reset()\n",
        "\n",
        "        # 게임 시작 전 랜덤하게 선공/후공 결정\n",
        "        if random_first:\n",
        "            agent_first = np.random.choice([True, False])\n",
        "            print(f\"===Agent goes {'first (O)' if agent_first else 'second (X)'}===\")\n",
        "        else:\n",
        "            agent_first = True  # 기본값은 에이전트가 선공\n",
        "\n",
        "        env.render(state)\n",
        "\n",
        "        while not state.is_done():\n",
        "            is_agent_turn = (state.is_going_first() == agent_first)\n",
        "\n",
        "            if is_agent_turn:  # Agent's turn\n",
        "                player = 'Agent\\'s'\n",
        "                policy = agent.get_policy(state)\n",
        "                action = np.random.choice(len(policy), p=policy)\n",
        "            else:  # Opponent's turn\n",
        "                if human_play:\n",
        "                    # 사람의 입력을 받아서 동작\n",
        "                    player = 'Your'\n",
        "                    print(\"0 1 2\\n3 4 5\\n6 7 8\")\n",
        "                    action = int(input(\"Enter your move : \"))\n",
        "                    while action not in state.available_actions:\n",
        "                        action = int(input(\"Invalid move. Try again : \"))\n",
        "                else:\n",
        "                    # 랜덤한 상대\n",
        "                    player = 'Random player\\'s'\n",
        "                    action = np.random.choice(state.available_actions)\n",
        "\n",
        "            state, _, _, _ = env.step(action)\n",
        "            print(f\"{player} turn\")\n",
        "            env.render(state)\n",
        "            print(\"---------------------\")\n",
        "\n",
        "        # 게임 결과 처리\n",
        "        if state.is_lose():\n",
        "            if (state.is_going_first() == agent_first):\n",
        "                print(\"Opponent wins!\\n\")\n",
        "                losses += 1\n",
        "            else:\n",
        "                print(\"Agent wins!\\n\")\n",
        "                agent_wins += 1\n",
        "        elif state.is_draw():\n",
        "            print(\"It's a draw!\\n\")\n",
        "            draws += 1\n",
        "\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Agent Wins: {agent_wins}\")\n",
        "    print(f\"Draws: {draws}\")\n",
        "    print(f\"Losses: {losses}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCmoc0a5K__A"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "agent = AlphaZeroAgent(best_net, env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(agent, env, num_games=10, human_play=False)"
      ],
      "metadata": {
        "id": "g_wuGzgerbsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb176ccc-5632-4ccc-ad8b-f8021607636a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAME 1/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X O\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X O\n",
            "- X -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O X O\n",
            "- X -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X O\n",
            "- X X\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O X O\n",
            "- X X\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X O\n",
            "X X X\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "GAME 2/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X -\n",
            "O - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X X -\n",
            "O - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "X X O\n",
            "O - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X X O\n",
            "O - X\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "X X O\n",
            "O O X\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X X O\n",
            "O O X\n",
            "X O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "X X O\n",
            "O O X\n",
            "X O O\n",
            "\n",
            "---------------------\n",
            "It's a draw!\n",
            "\n",
            "GAME 3/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X -\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X X\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X X\n",
            "- - O\n",
            "O O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X X X\n",
            "- - O\n",
            "O O -\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "GAME 4/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "O - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "O - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "O O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O X -\n",
            "- - -\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X -\n",
            "X - -\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O X -\n",
            "X O -\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X X\n",
            "X O -\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O X X\n",
            "X O O\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "It's a draw!\n",
            "\n",
            "GAME 5/10\n",
            "===Agent goes first (O)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Agent's turn\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- O -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O O -\n",
            "X - X\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O O\n",
            "X - X\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "GAME 6/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - O\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - O\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X -\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X X\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X X\n",
            "- - O\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X X X\n",
            "- - O\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "GAME 7/10\n",
            "===Agent goes first (O)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Agent's turn\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- O -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O -\n",
            "X - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O O -\n",
            "X - -\n",
            "- - X\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O O\n",
            "X - -\n",
            "- - X\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "GAME 8/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- O -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- - -\n",
            "- O X\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- - O\n",
            "- O X\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- - O\n",
            "- O X\n",
            "X - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- - O\n",
            "O O X\n",
            "X - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- - O\n",
            "O O X\n",
            "X X -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- - O\n",
            "O O X\n",
            "X X O\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "X - O\n",
            "O O X\n",
            "X X O\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "X O O\n",
            "O O X\n",
            "X X O\n",
            "\n",
            "---------------------\n",
            "It's a draw!\n",
            "\n",
            "GAME 9/10\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Random player's turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X O\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X O\n",
            "- X -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X O\n",
            "- X -\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X O\n",
            "- X X\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- X O\n",
            "- X X\n",
            "O O O\n",
            "\n",
            "---------------------\n",
            "Opponent wins!\n",
            "\n",
            "GAME 10/10\n",
            "===Agent goes first (O)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Agent's turn\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "- O -\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O -\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Random player's turn\n",
            "\n",
            "O O -\n",
            "X X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O O\n",
            "X X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent wins!\n",
            "\n",
            "\n",
            "Test Results:\n",
            "Agent Wins: 6\n",
            "Draws: 3\n",
            "Losses: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6SkUdu9-mJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fc0fdb-b454-42a5-9455-f568788193f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAME 1/3\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 7\n",
            "Your turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 2\n",
            "Your turn\n",
            "\n",
            "- X O\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X O\n",
            "- X -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 8\n",
            "Your turn\n",
            "\n",
            "- X O\n",
            "- X -\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X O\n",
            "- X X\n",
            "- O O\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 6\n",
            "Your turn\n",
            "\n",
            "- X O\n",
            "- X X\n",
            "O O O\n",
            "\n",
            "---------------------\n",
            "Opponent wins!\n",
            "\n",
            "GAME 2/3\n",
            "===Agent goes first (O)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "Agent's turn\n",
            "\n",
            "- O -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 4\n",
            "Your turn\n",
            "\n",
            "- O -\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O -\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 2\n",
            "Your turn\n",
            "\n",
            "O O X\n",
            "- X -\n",
            "- - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O X\n",
            "- X -\n",
            "O - -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 3\n",
            "Your turn\n",
            "\n",
            "O O X\n",
            "X X -\n",
            "O - -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "O - -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 8\n",
            "Your turn\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "O - X\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O O X\n",
            "X X O\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "It's a draw!\n",
            "\n",
            "GAME 3/3\n",
            "===Agent goes second (X)===\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 7\n",
            "Your turn\n",
            "\n",
            "- - -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X -\n",
            "- - -\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 5\n",
            "Your turn\n",
            "\n",
            "- X -\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "- X X\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 0\n",
            "Your turn\n",
            "\n",
            "O X X\n",
            "- - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X X\n",
            "X - O\n",
            "- O -\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 6\n",
            "Your turn\n",
            "\n",
            "O X X\n",
            "X - O\n",
            "O O -\n",
            "\n",
            "---------------------\n",
            "Agent's turn\n",
            "\n",
            "O X X\n",
            "X - O\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "0 1 2\n",
            "3 4 5\n",
            "6 7 8\n",
            "Enter your move : 4\n",
            "Your turn\n",
            "\n",
            "O X X\n",
            "X O O\n",
            "O O X\n",
            "\n",
            "---------------------\n",
            "It's a draw!\n",
            "\n",
            "\n",
            "Test Results:\n",
            "Agent Wins: 0\n",
            "Draws: 2\n",
            "Losses: 1\n"
          ]
        }
      ],
      "source": [
        "test(agent, env, num_games=3, human_play=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
